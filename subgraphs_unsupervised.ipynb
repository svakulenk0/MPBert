{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "split = 'train_set'  # dataset split\n",
    "kg = 'wikidata2020'  # background KG serialised in HDT\n",
    "limit = 2  # number of samples\n",
    "k = 10  # number of top matched predicates for MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subgraphs\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.spatial.distance\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdt_utils import HDT_Graph\n",
    "\n",
    "from predicates_dictionary import predicates\n",
    "from settings import data_path\n",
    "\n",
    "_dir = '%s/subgraphs/%s/' % (data_path, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph and transformer\n",
    "wikidata = HDT_Graph(kg)\n",
    "model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')  # predicate/question scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load adjacencies\n",
    "def generate_adj_sp(adjacencies, n_entities, include_inverse):\n",
    "    '''\n",
    "    Build adjacency matrix\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    \n",
    "    # colect all predicate matrices separately into a list\n",
    "    sp_adjacencies = []\n",
    "    for edges in adjacencies:\n",
    "        # split subject (row) and object (col) node URIs\n",
    "        n_edges = len(edges)\n",
    "        row, col = np.transpose(edges)\n",
    "        \n",
    "        # duplicate edges in the opposite direction\n",
    "        if include_inverse:\n",
    "            _row = np.hstack([row, col])\n",
    "            col = np.hstack([col, row])\n",
    "            row = _row\n",
    "            n_edges *= 2\n",
    "        \n",
    "        # create adjacency matrix for this predicate\n",
    "        data = np.ones(n_edges)\n",
    "        adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)\n",
    "        sp_adjacencies.append(adj)\n",
    "    \n",
    "    return np.asarray(sp_adjacencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q185449\n",
      "Subgraph with 10999 entities and 494 predicates\n",
      "\n",
      "\n",
      "Genre of the book Eragon?\n",
      "young adult literature;speculative fiction novel\n",
      "['http://www.wikidata.org/entity/Q1233720', 'http://www.wikidata.org/entity/Q10992055']\n",
      "The Encyclopedia of Science Fiction ID\n",
      "ISFDB title ID\n",
      "ISFDB author ID\n",
      "ISFDB publisher ID\n",
      "Discogs artist ID\n",
      "genre\n",
      "Encyclopædia Britannica Online ID\n",
      "narrative location\n",
      "Brockhaus Enzyklopädie online ID\n",
      "Library of Congress authority ID\n",
      "0.4277104437351227\n",
      "http://www.wikidata.org/entity/Q10992055\n",
      "http://www.wikidata.org/entity/Q1233720\n",
      "\n",
      "\n",
      "and who wrote it ?\n",
      "Christopher Paolini\n",
      "['http://www.wikidata.org/entity/Q93620']\n",
      "creator\n",
      "author\n",
      "ISFDB author ID\n",
      "narrative location\n",
      "country of origin\n",
      "occupation\n",
      "place of birth\n",
      "Babelio author ID\n",
      "after a work by\n",
      "country of citizenship\n",
      "0.436457097530365\n",
      "http://www.wikidata.org/entity/Q93620\n",
      "\n",
      "\n",
      "and when was he born ?\n",
      "17 November 1983\n",
      "When was the book published ?\n",
      "26 August 2003\n",
      "and who published the book ?\n",
      "Alfred A. Knopf\n",
      "['http://www.wikidata.org/entity/Q1431868']\n",
      "publisher\n",
      "published in\n",
      "author\n",
      "number of pages\n",
      "Babelio author ID\n",
      "publication date\n",
      "ISFDB author ID\n",
      "page(s)\n",
      "PubMed ID\n",
      "ResearchGate publication identifier\n",
      "0.561411440372467\n",
      "http://www.wikidata.org/entity/Q1431868\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-8eb92907fc49>:57: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  _A = A[[top_ids]]\n"
     ]
    }
   ],
   "source": [
    "# iterate over samples\n",
    "for file_name in os.listdir(_dir)[:limit]:\n",
    "    with open(_dir + file_name, \"r\") as data:\n",
    "        conversation = json.load(data)\n",
    "        seed_entity = conversation['seed_entity']\n",
    "        print(seed_entity)\n",
    "        \n",
    "        entity_ids = conversation['entities']\n",
    "        predicate_ids = conversation['predicates']\n",
    "        adjacencies = conversation['adjacencies']\n",
    "        n_entities = len(entity_ids)\n",
    "        print(\"Subgraph with %d entities and %d predicates\" % (n_entities, len(predicate_ids))) \n",
    "        \n",
    "        A = generate_adj_sp(adjacencies, n_entities, include_inverse=True)\n",
    "        \n",
    "        # seed activation\n",
    "        x = np.zeros(n_entities)\n",
    "        idx = entity_ids.index(conversation['seed_entity_id'])\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # get all labels for predicates in the graph\n",
    "        ps = {predicates[p]: i for i, p in enumerate(predicate_ids) if p in predicates}\n",
    "        p_labels = list(ps.keys())\n",
    "        # encode all predicates\n",
    "        p_vectors = model.encode(p_labels)\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "        for i, es in enumerate(conversation['answer_entities']):\n",
    "            print(conversation['questions'][i])\n",
    "            print(conversation['answer_texts'][i])\n",
    "            # answer entities\n",
    "            if es:\n",
    "                print(es)\n",
    "                p = conversation['questions'][i]\n",
    "                # encode question\n",
    "                q_vector = model.encode([p])[0]\n",
    "                \n",
    "                # compare question to all predicates in the graph\n",
    "                dists = []\n",
    "                for i, p_vector in enumerate(p_vectors):\n",
    "                    dists.append(scipy.spatial.distance.cosine(q_vector, p_vector))\n",
    "                assert len(dists) == len(p_labels)\n",
    "                \n",
    "                # get top-k scored predicates\n",
    "                top_k = sorted(range(len(dists)), key=lambda k: dists[k])[:k]\n",
    "                top_ids = []\n",
    "                p = []\n",
    "                for i in top_k:\n",
    "                    top_label = p_labels[i]\n",
    "                    print(top_label)\n",
    "                    top_ids.append(ps[top_label])\n",
    "                    p.append(1 - dists[i])\n",
    "                p = np.array(p)\n",
    "                \n",
    "                # select only the adjacency matrices for the top-k predicates in the subgraph\n",
    "                _A = A[[top_ids]]\n",
    "                _y = x @ sum(p*_A)\n",
    "                top = np.argwhere(_y > 0).T.tolist()[0]\n",
    "                results = defaultdict(list)\n",
    "                if len(top) > 0:\n",
    "                    activations = np.asarray(entity_ids)[top]\n",
    "                    for i, _id in enumerate(activations):\n",
    "                        uri = wikidata.look_up_uri(_id, 'entity')\n",
    "                        if uri:\n",
    "                            score = _y[top[i]]\n",
    "                            results[score].append(uri)\n",
    "                    \n",
    "                    # sort results\n",
    "                    results = sorted(results.items(), reverse=True)\n",
    "                    for score, uris in results:\n",
    "                        print(score)\n",
    "                        for u in uris:\n",
    "                            print(u)\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    print('No answer')\n",
    "                print('\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
