{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Hustle\n",
      "http://www.wikidata.org/entity/Q9013673\n",
      "\n",
      "\n",
      "Does Bradley Cooper star in American Hustle?\n",
      "Yes\n",
      "When was he born?\n",
      "5 January 1975\n",
      "And who composed?\n",
      "Danny Elfman\n",
      "http://www.wikidata.org/entity/Q193338\n",
      "Did the movie win a Golden Globe award?\n",
      "Yes\n",
      "How long is the movie?\n",
      "138 minute\n",
      "\n",
      "2 matched entities\n",
      "Subgraph with 1160 entities and 410 predicates\n"
     ]
    }
   ],
   "source": [
    "# load a subgraph\n",
    "import json\n",
    "\n",
    "path = '/ivi/ilps/personal/svakule'\n",
    "split = 'train_set'\n",
    "conv_id = 1\n",
    "\n",
    "qa_pairs = []\n",
    "with open('%s/subgraphs/%s/%d.json' % (path, split, conv_id), \"r\") as data:\n",
    "    conversation = json.load(data)\n",
    "    seed_entity = conversation['seed_entity']\n",
    "    print(conversation['seed_entity_text'])\n",
    "    print(seed_entity)\n",
    "    print('\\n')\n",
    "    \n",
    "    for i, es in enumerate(conversation['answer_entities']):\n",
    "        print(conversation['questions'][i])\n",
    "        print(conversation['answer_texts'][i])\n",
    "        for e in es:\n",
    "            if e:\n",
    "                print(e)\n",
    "                qa_pairs.append([seed_entity, conversation['questions'][i], e])\n",
    "    \n",
    "    matched_entity_ids = [conversation['seed_entity_id']] + [a for _as in conversation['answer_ids'] for a in _as if a]\n",
    "    print(\"\\n%d matched entities\" % len(matched_entity_ids))\n",
    "    \n",
    "    entity_ids = conversation['entities']\n",
    "    predicate_ids = conversation['predicates']\n",
    "    adjacencies = conversation['adjacencies']\n",
    "    n_entities = len(entity_ids)\n",
    "    print(\"Subgraph with %d entities and %d predicates\" % (n_entities, len(predicate_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "\n",
    "## Check connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 matched entities are connected with an edge\n",
      "355\n",
      "14078\n"
     ]
    }
   ],
   "source": [
    "# check the answer entity is reachable from the seed entity\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# load adjacencies\n",
    "def generate_adj_sp(adjacencies, n_entities, include_inverse):\n",
    "    '''\n",
    "    Build adjacency matrix\n",
    "    '''\n",
    "    adj_shape = (n_entities, n_entities)\n",
    "    \n",
    "    # colect all predicate matrices separately into a list\n",
    "    sp_adjacencies = []\n",
    "    for edges in adjacencies:\n",
    "        # split subject (row) and object (col) node URIs\n",
    "        n_edges = len(edges)\n",
    "        row, col = np.transpose(edges)\n",
    "        \n",
    "        # duplicate edges in the opposite direction\n",
    "        if include_inverse:\n",
    "            _row = np.hstack([row, col])\n",
    "            col = np.hstack([col, row])\n",
    "            row = _row\n",
    "            n_edges *= 2\n",
    "        \n",
    "        # create adjacency matrix for this predicate\n",
    "        data = np.ones(n_edges)\n",
    "        adj = sp.csr_matrix((data, (row, col)), shape=adj_shape)\n",
    "        sp_adjacencies.append(adj)\n",
    "    \n",
    "    return np.asarray(sp_adjacencies)\n",
    "\n",
    "# seed activation\n",
    "x = np.zeros(n_entities)\n",
    "for e in matched_entity_ids:\n",
    "    idx = entity_ids.index(e)\n",
    "    x[idx] = 1\n",
    "\n",
    "A = generate_adj_sp(adjacencies, n_entities, include_inverse=True)\n",
    "\n",
    "for i, _A in enumerate(A):\n",
    "    # MP\n",
    "    _y = x @ _A\n",
    "  \n",
    "    # report edges between matched entities\n",
    "    z = _y * x\n",
    "    overlap = int(sum(z))\n",
    "    if overlap:\n",
    "        print(\"%d matched entities are connected with an edge\" % int(overlap))\n",
    "        print(i)\n",
    "        print(predicate_ids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nationale Thesaurus voor Auteurs ID': 185, 'film editor': 186, 'occupation': 187, 'Box Office Mojo film ID': 188, 'Rotten Tomatoes ID': 189, 'NNDB people ID': 190, 'AlloCiné film ID': 191, 'AlloCiné person ID': 192, 'CANTIC ID': 193, 'instrument': 194, 'NLA Trove ID': 195, 'described by source': 196, 'winner': 197, 'genre': 198, 'religion': 199, 'nominated for': 200, 'languages spoken, written or signed': 201, 'Encyclopædia Britannica Online ID': 202, 'executive producer': 203, 'published in': 204, 'based on': 205, 'title': 206, 'birth name': 207, 'has quality': 208, 'name in native language': 209, 'AllMovie movie ID': 210, 'University of Barcelona authority ID': 211, 'cast member': 212, 'producer': 213, 'award received': 214, 'country': 215, 'Metacritic ID': 216, 'AllMusic artist ID': 217, 'GTAA ID': 218, 'performer': 219, 'DNF film ID': 220, 'genealogics.org person ID': 221, 'Netflix ID': 222, 'place of birth': 223, 'Spotify artist ID': 224, 'Discogs artist ID': 225, 'MovieMeter film ID': 226, 'number of children': 227, 'Les Archives du Spectacle Person ID': 228, 'FSK film rating': 229, 'Twitter username': 230, 'Facebook ID': 231, 'AllMovie person ID': 232, 'work period (start)': 233, 'duration': 234, 'sex or gender': 235, 'International Standard Name Identifier': 236, 'cost': 237, 'VIAF ID': 238, 'box office': 239, 'FAST ID': 240, 'Swedish Film Database person ID': 241, 'Museum of Modern Art artist ID': 242, 'father': 243, 'GND ID': 244, 'Swedish Film Database film ID': 245, 'Elonet movie ID': 246, 'NMHH film rating': 247, 'Genius artist ID': 248, 'Elonet person ID': 249, 'PORT person ID': 250, 'Library of Congress authority ID': 251, 'Allcinema film ID': 252, 'mother': 253, 'KINENOTE film ID': 254, 'costume designer': 255, 'Scope.dk film ID': 256, 'Scope.dk person ID': 257, 'ČSFD film ID': 258, 'production designer': 259, 'spouse': 260, 'Kinopoisk film ID': 261, 'Kinopoisk person ID': 262, 'ČSFD person ID': 263, 'Danish National Filmography person ID': 264, 'TCM Movie Database film ID': 265, 'Filmportal ID': 266, 'record label': 267, 'Bibliothèque nationale de France ID': 268, 'Box Office Mojo person ID': 269, 'IdRef ID': 270, 'country of citizenship': 271, 'production company': 272, 'iTunes artist ID': 273, 'cites work': 274, 'Great Russian Encyclopedia Online ID': 275, \"category's main topic\": 276, 'SoundCloud ID': 277, 'TCM Movie Database person ID': 278, 'Cineplex film ID': 279, 'instance of': 280, 'cine.gr film ID': 281, 'elCinema film ID': 282, 'OFDb ID': 283, 'EDb film ID': 284, 'elFilm film ID': 285, 'Sratim ID': 286, 'Last.fm ID': 287, 'MySpace ID': 288, 'Open Media Database film ID': 289, 'KINENOTE person ID': 290, 'ICAA rating': 291, 'sibling': 292, 'CNC film rating (Romania)': 293, 'Quora topic ID': 294, 'SNAC Ark ID': 295, 'VGMdb artist ID': 296, 'director of photography': 297, 'IMDb ID': 298, 'Songkick artist ID': 299, 'discography': 300, 'original language of film or TV show': 301, 'JMK film rating': 302, 'Commons category': 303, 'RTC film rating': 304, 'Deutsche Synchronkartei film ID': 305, 'SBN author ID': 306, 'child': 307, 'Billboard artist ID': 308, 'Cinémathèque québécoise work identifier': 309, 'issue': 310, 'MusicBrainz artist ID': 311, 'Douban film ID': 312, 'color': 313, 'Bechdel Test Movie List ID': 314, 'The Peerage person ID': 315, 'The Numbers person ID': 316, 'CineMagia person ID': 317, 'volume': 318, 'FilmAffinity ID': 319, 'TMDb movie ID': 320, 'country of origin': 321, 'TMDb person ID': 322, 'National Library of Korea Identifier': 323, 'Il mondo dei doppiatori ID': 324, 'Mormon Literature and Creative Arts Database artist ID': 325, 'Open Media Database person ID': 326, 'date of birth': 327, 'director': 328, 'publication date': 329, 'Moviepilot.de film ID': 330, 'screenwriter': 331, 'Medierådet rating': 332, 'Academy Awards Database film ID': 333, 'Academy Awards Database nominee ID': 334, 'Disney A to Z ID': 335, 'epoch': 336, 'Freebase ID': 337, \"Rock's Backpages artist ID\": 338, 'WhoSampled artist ID': 339, 'Google Play Movies & TV ID': 340, 'lyrics by': 341, 'Hoopla artist ID': 342, 'educated at': 343, 'NKCR AUT ID': 344, 'Musicalics composer ID': 345, 'PubMed ID': 346, 'Allcinema person ID': 347, 'voice actor': 348, 'family name': 349, 'given name': 350, 'influenced by': 351, 'distributor': 352, 'Europeana entity': 353, 'narrative location': 354, 'composer': 355, 'PORT film ID': 356, \"topic's main category\": 357, 'filming location': 358, 'PMCID': 359, 'work location': 360, 'Biblioteca Nacional de España ID': 361, 'category combines topics': 362}\n"
     ]
    }
   ],
   "source": [
    "from predicates_dictionary import predicates\n",
    "\n",
    "# get all labels for predicates in the graph\n",
    "ps = {predicates[p]: i for i, p in enumerate(predicate_ids) if p in predicates}\n",
    "print(ps)\n",
    "p_labels = list(ps.keys())\n",
    "# print(p_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['http://www.wikidata.org/entity/Q9013673', 'And who composed?', 'http://www.wikidata.org/entity/Q193338']]\n",
      "And who composed?\n"
     ]
    }
   ],
   "source": [
    "# P86 composer 177\n",
    "print(qa_pairs)\n",
    "p = qa_pairs[0][1]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all predicates\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
    "\n",
    "sentence_embeddings = model.encode([p] + p_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance\n",
    "\n",
    "q_vector = sentence_embeddings[0]\n",
    "dists = []\n",
    "for i, p_vector in enumerate(sentence_embeddings[1:]):\n",
    "    dists.append(scipy.spatial.distance.cosine(q_vector, p_vector))\n",
    "assert len(dists) == len(p_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "composer\n",
      "355\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "top_k = sorted(range(len(dists)), key=lambda k: dists[k])[:k]\n",
    "for i in top_k:\n",
    "    top_label = p_labels[i]\n",
    "    print(top_label)\n",
    "    top_id = ps[top_label]\n",
    "    print(top_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "from hdt_utils import HDT_Graph\n",
    "wikidata = HDT_Graph('wikidata2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q193338\n"
     ]
    }
   ],
   "source": [
    "# MP to find the answer in the graph propagate only over the top-matched predicate\n",
    "\n",
    "# seed activation\n",
    "x = np.zeros(n_entities)\n",
    "idx = entity_ids.index(conversation['seed_entity_id'])\n",
    "x[idx] = 1\n",
    "\n",
    "# find adjacency matrix for the predicate in the subgraph\n",
    "_A = A[top_id]\n",
    "# MP\n",
    "_y = x @ _A\n",
    "top = np.argwhere(_y > 0).T.tolist()[0]\n",
    "if len(top) > 0:\n",
    "    activations = np.asarray(entity_ids)[top]\n",
    "    for _id in activations:\n",
    "        uri = wikidata.look_up_uri(_id, 'entity')\n",
    "        print(uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
